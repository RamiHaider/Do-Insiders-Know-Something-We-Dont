{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJZH9MZ+p3R95hMtFZrtDR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","Place holder for our introduction to insider trading and the benefit of using successful insider trades."],"metadata":{"id":"5I8RH3jNsUX-"}},{"cell_type":"markdown","source":["## Libraries and Dependencies\n","In order to increase the repeatability, reproducibility, and replicability of our project, we will load in all of our libraries and freeze the dependencies to a file so that anyone replicating our research will know the versions used."],"metadata":{"id":"sI5192-FwA22"}},{"cell_type":"code","source":["#Colab Libraries\n","from google.colab import drive, files\n","#Data Import Libraries\n","import os, zipfile, time, requests\n","from bs4 import BeautifulSoup\n","#Data Manipulation Libraries\n","import numpy as np\n","import pandas as pd\n","#Visualization Libraries\n","import matplotlib.pyplot as plt"],"metadata":{"id":"9vRBl823v_eJ","executionInfo":{"status":"ok","timestamp":1747259879154,"user_tz":360,"elapsed":137,"user":{"displayName":"Thomas Macpherson","userId":"08726222529195553341"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#print the dependencies in the notebook\n","!pip freeze\n","\n","#create a .txt file that contains all versions\n","#!pip freeze > colab_requirements.txt"],"metadata":{"id":"UxA6Mer2xVqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading Primary Dataset\n","We will load multiple SEC Form 4 filing zip archives (Source: https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets). Each ZIP archive contains 10 files, we will extract and process three .tsv files inside of each archive and filter insider transactions by open-market purchases transacted by individual insiders (excluding investment entitities such as funds, limited parnerships, and trusts). We will identify transactions involving corporate officers and clen the data by removing all invalid records (those with missing roles). The processed results are compiled into a dataframe and saved to a .csv for backup and potential upload to a database or machine-learning pipeline (e.g. BigQuery).\n","\n","We will start by mounting our google drive and importing files."],"metadata":{"id":"4NLvWyrSsZW1"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osHDo6x5sSdz","executionInfo":{"status":"ok","timestamp":1747259448252,"user_tz":360,"elapsed":19780,"user":{"displayName":"Thomas Macpherson","userId":"08726222529195553341"}},"outputId":"5a1fb2a1-cd0e-4f76-f50c-d7d5afde6007"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount google drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["'''For the final project, we will have only a singular path to all infomation'''\n","#Students Google Drive Path\n","toms_path = '/content/drive/MyDrive/Colab Notebooks/593 - Milestone I/593 - Insider Trading Milestone I Project'\n","kirts_path = None\n","ramis_path = None\n","\n","#Navigate to the right working directory and confirm our current working drive\n","os.chdir(toms_path)\n","#os.chdir(kirts_path)\n","#os.chdir(ramis_path)\n","print(os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvhpAYHyvv85","executionInfo":{"status":"ok","timestamp":1747259465543,"user_tz":360,"elapsed":965,"user":{"displayName":"Thomas Macpherson","userId":"08726222529195553341"}},"outputId":"8a8a806b-e408-4471-8dcf-36759ae31b99"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/593 - Milestone I/593 - Insider Trading Milestone I Project\n"]}]},{"cell_type":"markdown","source":["The first thing that we need to do is go to the SEC website and download all of the ZIP archives of the data and save them to our google drive. You must identify yourself with a proper User-Agent header in order to connect to the SEC website"],"metadata":{"id":"NIyPpmUXychD"}},{"cell_type":"code","source":["'''Only run this cell once to download the data'''\n","\n","#URLs where we can download the files\n","url = 'https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets'\n","#Create a session with a real User-Agent\n","session = requests.Session()\n","session.headers.update({'User-Agent':'tmacphe (tmacpe@umich.edu)',\n","                       'Accept-Encoding': 'gzip,deflate',\n","                       'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'})\n","#Let's grab the page\n","page = session.get(url)\n","page.raise_for_status()\n","\n","#Now, we need to find all of the .zip links\n","soup = BeautifulSoup(page.content, 'html.parser')\n","zip_links = []\n","for a in soup.find_all('a',href=True):\n","    href = a['href']\n","    if href.lower().endswith('.zip'):\n","        url = href if href.startswith('http') else 'https://www.sec.gov' + href\n","        zip_links.append(url)\n","\n","#Create a folder to store all of our zipped archives\n","os.makedirs('sec_insider_zips',exist_ok=True)\n","\n","#Download each file into the directory\n","for url in zip_links:\n","    #We can pull out the file name using the os.path\n","    filename = os.path.basename(url)\n","    out_path = os.path.join('sec_insider_zips',filename)\n","    if os.path.exists(out_path):\n","        print(f\"Skipping {filename} because it has already been downloaded\")\n","        continue\n","    else:\n","        print(f\"Downloading {filename}...\")\n","        #Now let's get the new webpages with the zip files\n","        zip_file = session.get(url)\n","        zip_file.raise_for_status()\n","        #create a file and write the contents to it (write binary 'wb')\n","        with open(out_path, 'wb') as f:\n","            f.write(zip_file.content)\n","        print(f\"{filename} downloaded\")\n","        time.sleep(0.5)\n"],"metadata":{"id":"nbh4vhBnzgJg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next I will add all of Kirt's data manipulation for merging the files."],"metadata":{"id":"5FVsO2N174a7"}}]}